devtools::install_github("ropensci/RSelenium")
install.packages("XML")
install.packages("installr")
installr::updateR()
install.packages("RSelenium")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
install.packages("knitr")
library(RSelenium)
driver <- rsDriver(browser = "chrome", port = 80L, chromever = "83.0.4103.39")
driver <- remoteDriver(browser = "chrome", port = 80L, chromever = "83.0.4103.39")
driver <- remoteDriver(browser = "chrome", port = 80L)
driver
rmDr <- driver[["client"]]
rmDr$navigate("https://scholar.google.com/citations?user=7QTZbQgAAAAJ&hl")
rmDr
rmDr
driver[["client"]]
driver$open
driver$open()
driver <- remoteDriver(browser = "chrome")
driver$open()
getBibtex <- function(cite){
citation <- cite %>% sapply(
FUN=function(i) {
if(is.na(i)) return(NA)
html_session(
sprintf("https://scholar.google.com/scholar?q=info:%s:scholar.google.com/&output=cite&scirp=0&hl=en", i),
add_headers(userAgent),
set_cookies(newcookie)
) %>%
follow_link("BibTeX") %>%
extract2('response') %>%
as.character() %>%
extract(1) %>%
as.character()
})
}
id
get_publications(id)
install.packages("scholar")
install.packages("tidyverse")
get_publications(id)
library(scholar)
get_publications(id)
getBibtex(1yQoGdGgb4wC)
getBibtex("1yQoGdGgb4wC")
library(tidyverse)
getBibtex("1yQoGdGgb4wC")
library(rvest)
getBibtex("1yQoGdGgb4wC")
library(httr)
getBibtex("1yQoGdGgb4wC")
userAgent
oldUserAgent <- options("HTTPUserAgent")
options("HTTPUserAgent"="Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36")
userAgent
load("C:/Users/hboth/Downloads/sysdata.rda")
userAgent
newcookie
getBibtex("1yQoGdGgb4wC")
cite
cite="1yQoGdGgb4wC"
cite %>% sapply(
FUN=function(i) {
if(is.na(i)) return(NA)
html_session(
sprintf("https://scholar.google.com/scholar?q=info:%s:scholar.google.com/&output=cite&scirp=0&hl=en", i),
add_headers(),
set_cookies()
) %>%
follow_link("BibTeX") %>%
extract2('response') %>%
as.character() %>%
extract(1) %>%
as.character()
})
i=cite
i
html_session(
sprintf("https://scholar.google.com/scholar?q=info:%s:scholar.google.com/&output=cite&scirp=0&hl=en", i)
html_session(
sprintf("https://scholar.google.com/scholar?q=info:%s:scholar.google.com/&output=cite&scirp=0&hl=en", i),
add_headers(),
set_cookies()
)
html_session(
sprintf("https://scholar.google.com/scholar?q=info:%s:scholar.google.com/&output=cite&scirp=0&hl=en", i),
add_headers(),
set_cookies()
)
sprintf("https://scholar.google.com/scholar?q=info:%s:scholar.google.com/&output=cite&scirp=0&hl=en", i)
author="Hugo Botha"
googleScholarUrl <- "http://scholar.google.com"
# fake google id (looks like it is a 16 elements hex)
googleId <- sample(1000000:9999999, 1) %>% digest::digest(algo="md5") %>% stringr::str_sub(start=1, end=15)
# Set User-Agent to appear not to be a bot
oldUserAgent <- options("HTTPUserAgent")
options("HTTPUserAgent"="Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36")
institution=""
# putting together the search-URL:
query <- sprintf("author:'%s' %s", author, institution)
query
resultsURL <- paste0(
"http://scholar.google.com/scholar?q=",
query %>%
str_replace_all(pattern=" ", replacement="%20"))
resultsURL
message("The URL used is: ", "\n----\n", resultsURL)
# get content and parse it:
doc <- read_html(resultsURL)
doc
# number of hits:
h1 <- doc %>%
html_nodes(xpath="//div[@id='gs_ab_md']") %>%
html_text()
h1
h2 <- unlist(strsplit(h1, "\\s"))
h2
# in splitted string it is the second element which contains digits,
# grab it and remove decimal signs and convert to integer
num <- as.integer(gsub("[[:punct:]]", "", h2[grep("\\d", h2)[1]]))
num
message(paste0("\n\nNumber of hits: ", num, "\n----\n", "If this number is far from the returned results\nsomething might have gone wrong..\n\n", sep = ""))
# If there are no results, stop and throw an error message:
if (num == 0 | is.na(num)) {
stop("\n\n...There is no result for the submitted search string!")
}
pages.max <- ceiling(num/20)
pages.max
# 'start' as used in URL:
start <- 20 * 1:pages.max - 20
# Collect URLs as list:
URLs <- paste0(resultsURL, sprintf("&start=%d", start))
scraper_internal
#' Assemble the data from a page of google search responses
#'
#' @param URL link to google search responses
#' @importFrom magrittr extract
#' @importFrom magrittr extract2
#' @importFrom magrittr %>%
#' @importFrom rvest follow_link
#' @importFrom xml2 read_html
#' @importFrom rvest html_nodes
#' @importFrom rvest html_attr
#' @importFrom rvest html_session
#' @importFrom rvest html_children
#' @importFrom rvest html_name
#' @importFrom stringr str_extract
#' @importFrom stringr str_replace_all
#' @importFrom stringr str_replace
#' @importFrom httr add_headers
#' @importFrom httr set_cookies
scraper_internal <- function(URL) {
doc <- read_html(URL)
res <- html_nodes(doc, xpath="//div[@class='gs_ri']")
# citation:
citekey <- html_nodes(doc, xpath="//*/a[@class='gs_nph'][@onclick]") %>%
html_attr("onclick") %>%
str_extract("\\'[\\w-]{12}\\'") %>%
str_replace_all("\\'", "")
# Pull out db rows
cite <- citekey %>% getBibtex()
type <- str_extract(cite, "@(\\w{1,})") %>%
str_replace("@", "")
title <- str_extract(cite, "title=\\{.*?\\},?\\n") %>%
str_replace("title=\\{(.*)?\\},?\\n", "\\1")
booktitle <- str_extract(cite, "booktitle=\\{.*?\\},?\\n") %>%
str_replace("booktitle=\\{(.*)?\\},?\\n", "\\1")
authors <- str_extract(cite, "author=\\{.*?\\},?\\n") %>%
str_replace("author=\\{(.*)?\\},?\\n", "\\1")
journal <- str_extract(cite, "journal=\\{.*?\\},?\\n") %>%
str_replace("journal=\\{(.*)?\\},?\\n", "\\1")
publisher <- str_extract(cite, "publisher=\\{.*?\\},?\\n") %>%
str_replace("publisher=\\{(.*)?\\},?\\n", "\\1")
year <- str_extract(cite, "year=\\{.*?\\},?\\n") %>%
str_replace("year=\\{(.*)?\\},?\\n", "\\1") %>%
as.numeric()
# link:
link <- html_nodes(doc, xpath="//div[@class='gs_ri']/h3[@class='gs_rt']") %>%
lapply(FUN=function(i){
tmp <- html_children(i)
linkidx <- which(tmp %>% html_name()=='a')
if(length(linkidx)==0) return(NA)
tmp %>% extract2(linkidx[1]) %>%
html_attr('href') %>%
extract(1)
}) %>% unlist()
# summaries are truncated, and thus wont be used..
# abst <- xpathSApply(doc, '//div[@class='gs_rs']', xmlValue)
# ..to be extended for individual needs
options(warn=(-1))
dat <- data.frame(
citekey = citekey,
type = type,
title = title,
authors = authors,
journal = journal,
booktitle = booktitle,
publisher = publisher,
year = year,
link = link,
cite = cite
)
options(warn=0)
return(dat)
}
result <- do.call("rbind", lapply(URLs, scraper_internal))
URLs
t1="http://scholar.google.com/scholar?q=author:'Hugo%20Botha'%20&start=0"
URL=t1
doc <- read_html(URL)
options("HTTPUserAgent")
read_html
read_html(URL)
install.packages("RefManageR")
# Reset userAgent to previous value
options("HTTPUserAgent"=oldUserAgent)
library(RefManageR)
ReadGS(id,0,1)
ReadGS(id,0,3)
ReadGS(id,0,3,check.entries = FALSE)
ReadGS("7QTZbQgAAAAJ",0,3,check.entries = FALSE)
get_publications(id)
pbs=get_publications(id)
pbs[1]
pbs[1,1]
pbs[1,1:2]
pbs[1,]
query
query=paste(pbs[1,1], pbs[1,3])
query
query=paste(pbs[1,1], pbs[1,3] pbs[1,6])
query=paste(pbs[1,1], pbs[1,3], pbs[1,6])
ReadCrossRef(query)
ReadCrossRef(query,use.old.api = TRUE)
install_github('rmetadata', 'ropensci')
install.packages("devtools")
devtools::install_github('rmetadata', 'ropensci')
devtools::install_github('rmetadata')
devtools::install_github('https://github.com/ropensci/rmetadata')
tribble(
~Year, ~Name, ~Institution,
2016, "Mayo Brother's Fellowship Award", "Mayo Clinic",
) %>%
detailed_entries(
where = Year,
with = Name,
what = Institution
)
tribble(
~Year, ~Name, ~Institution,
2016, "Mayo Brother's Fellowship Award", "Mayo Clinic",
)
library(vitae)
install.packages("vitae")
library(vitae)
tribble(
~Year, ~Name, ~Institution,
2016, "Mayo Brother's Fellowship Award", "Mayo Clinic",
) %>%
detailed_entries(
where = Year,
with = Name,
what = Institution
)
install.packages("blogdown")
library(blogdown)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
library(blogdown)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
getwd()
install.packages("anytime")
bibtex_2academic <- function(bibfile,
outfold,
abstract = FALSE,
overwrite = FALSE) {
require(RefManageR)
require(dplyr)
require(stringr)
require(anytime)
# Import the bibtex file and convert to data.frame
mypubs   <- ReadBib(bibfile, check = "warn", .Encoding = "UTF-8") %>%
as.data.frame()
# assign "categories" to the different types of publications
mypubs   <- mypubs %>%
dplyr::mutate(
pubtype = dplyr::case_when(document_type == "Article" ~ "2",
document_type == "Article in Press" ~ "2",
document_type == "InProceedings" ~ "1",
document_type == "Proceedings" ~ "1",
document_type == "Conference" ~ "1",
document_type == "Conference Paper" ~ "1",
document_type == "MastersThesis" ~ "3",
document_type == "PhdThesis" ~ "3",
document_type == "Manual" ~ "4",
document_type == "TechReport" ~ "4",
document_type == "Book" ~ "5",
document_type == "InCollection" ~ "6",
document_type == "InBook" ~ "6",
document_type == "Misc" ~ "0",
TRUE ~ "0"))
# create a function which populates the md template based on the info
# about a publication
create_md <- function(x) {
# define a date and create filename by appending date and start of title
if (!is.na(x[["year"]])) {
x[["date"]] <- paste0(x[["year"]], "-01-01")
} else {
x[["date"]] <- "2999-01-01"
}
filename <- paste(x[["date"]], x[["title"]] %>%
str_replace_all(fixed(" "), "_") %>%
str_remove_all(fixed(":")) %>%
str_sub(1, 20) %>%
paste0(".md"), sep = "_")
# start writing
if (!file.exists(file.path(outfold, filename)) | overwrite) {
fileConn <- file.path(outfold, filename)
write("+++", fileConn)
# Title and date
write(paste0("title = \"", x[["title"]], "\""), fileConn, append = T)
write(paste0("date = \"", anydate(x[["date"]]), "\""), fileConn, append = T)
# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
auth_hugo <- str_replace_all(x["author"], " and ", "\", \"")
auth_hugo <- stringi::stri_trans_general(auth_hugo, "latin-ascii")
write(paste0("authors = [\"", auth_hugo,"\"]"), fileConn, append = T)
# Publication type. Legend:
# 0 = Uncategorized, 1 = Conference paper, 2 = Journal article
# 3 = Manuscript, 4 = Report, 5 = Book,  6 = Book section
write(paste0("publication_types = [\"", x[["pubtype"]],"\"]"),
fileConn, append = T)
# Publication details: journal, volume, issue, page numbers and doi link
publication <- x[["journal"]]
if (!is.na(x[["volume"]])) publication <- paste0(publication,
", (", x[["volume"]], ")")
if (!is.na(x[["number"]])) publication <- paste0(publication,
", ", x[["number"]])
if (!is.na(x[["pages"]])) publication <- paste0(publication,
", _pp. ", x[["pages"]], "_")
if (!is.na(x[["doi"]])) publication <- paste0(publication,
", ", paste0("https://doi.org/",
x[["doi"]]))
write(paste0("publication = \"", publication,"\""), fileConn, append = T)
write(paste0("publication_short = \"", publication,"\""),fileConn, append = T)
# Abstract and optional shortened version.
if (abstract) {
write(paste0("abstract = \"", x[["abstract"]],"\""), fileConn, append = T)
} else {
write("abstract = \"\"", fileConn, append = T)
}
write(paste0("abstract_short = \"","\""), fileConn, append = T)
# other possible fields are kept empty. They can be customized later by
# editing the created md
write("image_preview = \"\"", fileConn, append = T)
write("featured = false", fileConn, append = T)
write("projects = []", fileConn, append = T)
write("tags = []", fileConn, append = T)
#links
write("url_pdf = \"\"", fileConn, append = T)
write("url_preprint = \"\"", fileConn, append = T)
write("url_code = \"\"", fileConn, append = T)
write("url_dataset = \"\"", fileConn, append = T)
write("url_project = \"\"", fileConn, append = T)
write("url_slides = \"\"", fileConn, append = T)
write("url_video = \"\"", fileConn, append = T)
write("url_poster = \"\"", fileConn, append = T)
write("url_source = \"\"", fileConn, append = T)
#other stuff
write("math = true", fileConn, append = T)
write("highlight = true", fileConn, append = T)
# Featured image
write("[header]", fileConn, append = T)
write("image = \"\"", fileConn, append = T)
write("caption = \"\"", fileConn, append = T)
write("+++", fileConn, append = T)
}
}
# apply the "create_md" function over the publications list to generate
# the different "md" files.
apply(mypubs, FUN = function(x) create_md(x), MARGIN = 1)
}
bibtex_2academic("forsite.bib",outfold = "_temp_publications/")
require(RefManageR)
require(dplyr)
require(stringr)
require(anytime)
my_bibfile <- "~/GitHub/academic-kickstart/forsite.bib"
out_fold   <- "~/GitHub/academic-kickstart/_temp_publications/"
bibtex_2academic(my_bibfile, out_fold, FALSE, TRUE)
bibtex_2academic(my_bibfile, out_fold, FALSE, TRUE)
bibtex_2academic(my_bibfile, out_fold, FALSE, TRUE)
bibfile
bibfile <- "~/GitHub/academic-kickstart/forsite.bib"
outfold   <- "~/GitHub/academic-kickstart/_temp_publications/"
# Import the bibtex file and convert to data.frame
mypubs   <- ReadBib(bibfile, check = "warn", .Encoding = "UTF-8") %>%
as.data.frame()
# Import the bibtex file and convert to data.frame
mypubs   <- ReadBib(bibfile, check = "warn", .Encoding = "UTF-8") %>%
as.data.frame()
mypubs
# assign "categories" to the different types of publications
mypubs   <- mypubs %>%
dplyr::mutate(
pubtype = dplyr::case_when(document_type == "Article" ~ "2",
document_type == "Article in Press" ~ "2",
document_type == "InProceedings" ~ "1",
document_type == "Proceedings" ~ "1",
document_type == "Conference" ~ "1",
document_type == "Conference Paper" ~ "1",
document_type == "MastersThesis" ~ "3",
document_type == "PhdThesis" ~ "3",
document_type == "Manual" ~ "4",
document_type == "TechReport" ~ "4",
document_type == "Book" ~ "5",
document_type == "InCollection" ~ "6",
document_type == "InBook" ~ "6",
document_type == "Misc" ~ "0",
TRUE ~ "0"))
# create a function which populates the md template based on the info
# about a publication
create_md <- function(x) {
# define a date and create filename by appending date and start of title
if (!is.na(x[["year"]])) {
x[["date"]] <- paste0(x[["year"]], "-01-01")
} else {
x[["date"]] <- "2999-01-01"
}
foldername <- paste(x[["date"]], x[["title"]] %>%
str_replace_all(fixed(" "), "_") %>%
str_remove_all(fixed(":")) %>%
str_sub(1, 20), sep = "_")
#folder = paste0(outfold, "/", foldername)
dir.create(file.path(outfold, foldername), showWarnings = FALSE)
filename = "index.md"
# start writing
outsubfold = paste(outfold, foldername, sep="/")
# start writing
if (!file.exists(file.path(outsubfold, filename)) | overwrite) {
fileConn <- file.path(outsubfold, filename)
write("+++", fileConn)
# Title and date
write(paste0("title = \"", x[["title"]], "\""), fileConn, append = T)
write(paste0("date = \"", anydate(x[["date"]]), "\""), fileConn, append = T)
# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
auth_hugo <- str_replace_all(x["author"], " and ", "\", \"")
auth_hugo <- stringi::stri_trans_general(auth_hugo, "latin-ascii")
write(paste0("authors = [\"", auth_hugo,"\"]"), fileConn, append = T)
# Publication type. Legend:
# 0 = Uncategorized, 1 = Conference paper, 2 = Journal article
# 3 = Manuscript, 4 = Report, 5 = Book,  6 = Book section
write(paste0("publication_types = [\"", x[["pubtype"]],"\"]"),
fileConn, append = T)
# Publication details: journal, volume, issue, page numbers and doi link
publication <- x[["journal"]]
if (!is.na(x[["volume"]])) publication <- paste0(publication,
", (", x[["volume"]], ")")
if (!is.na(x[["number"]])) publication <- paste0(publication,
", ", x[["number"]])
if (!is.na(x[["pages"]])) publication <- paste0(publication,
", _pp. ", x[["pages"]], "_")
if (!is.na(x[["doi"]])) publication <- paste0(publication,
", ", paste0("https://doi.org/",
x[["doi"]]))
write(paste0("publication = \"", publication,"\""), fileConn, append = T)
write(paste0("publication_short = \"", publication,"\""),fileConn, append = T)
# Abstract and optional shortened version.
if (abstract) {
write(paste0("abstract = \"", x[["abstract"]],"\""), fileConn, append = T)
} else {
write("abstract = \"\"", fileConn, append = T)
}
write(paste0("abstract_short = \"","\""), fileConn, append = T)
# other possible fields are kept empty. They can be customized later by
# editing the created md
write("image_preview = \"\"", fileConn, append = T)
write("selected = false", fileConn, append = T)
write("projects = []", fileConn, append = T)
write("tags = []", fileConn, append = T)
#links
write(paste0("url_pdf = \"", x[["url"]],"\""), fileConn, append = T)
write("url_preprint = \"\"", fileConn, append = T)
write("url_code = \"\"", fileConn, append = T)
write("url_dataset = \"\"", fileConn, append = T)
write("url_project = \"\"", fileConn, append = T)
write("url_slides = \"\"", fileConn, append = T)
write("url_video = \"\"", fileConn, append = T)
write("url_poster = \"\"", fileConn, append = T)
write("url_source = \"\"", fileConn, append = T)
#other stuff
write("math = true", fileConn, append = T)
write("highlight = true", fileConn, append = T)
# Featured image
write("[header]", fileConn, append = T)
write("image = \"\"", fileConn, append = T)
write("caption = \"\"", fileConn, append = T)
write("+++", fileConn, append = T)
}
# convert entry back to data frame
df_entry = as.data.frame(as.list(x), stringsAsFactors=FALSE) %>%
column_to_rownames("rowname")
# write cite.bib file to outsubfolder
WriteBib(as.BibEntry(df_entry[1,]), paste(outsubfold, "cite.bib", sep="/"))
}
